# Corteza - User Feedback & Product Insights

This document tracks feedback from demos, user interviews, and real-world usage to inform product direction.

---

## Week of December 16, 2024

### Source: Product Demos (2 demos this week)

#### üéØ Key Feedback

**Problem 1: High Onboarding Friction**
- Too much manual uplift work required from users
- Users need to manually add decisions OR upload files
- Requires significant effort to populate the system
- Users need to change their workflow to adopt the tool

**Problem 2: Delayed Value Realization**
- Value proposition depends on having enough historical data
- Learning from past decisions requires critical mass of logged decisions
- New teams see empty system ‚Üí don't immediately see the benefit
- Time lag between starting to use the tool and realizing its value

#### üí° Potential Solutions Discussed

**Reduce Onboarding Friction:**
1. AI-powered passive decision detection (bot monitors Slack, auto-detects decisions)
2. Bulk import from existing sources (Slack history, Jira, Confluence)
3. Integration triggers (prompt when Jira ticket closes, PR merges, meeting ends)

**Show Value Faster:**
1. Decision templates & best practices (provide value from day 1)
2. Team alignment dashboard (decision velocity, distribution metrics)
3. Demo mode with sample data (show what insights are possible)
4. Decision reversal tracking (learn from changed decisions even with small dataset)

#### üìä Impact
- Critical for adoption
- Identifies the gap between "nice to have" and "can't live without"
- Need more feedback to validate patterns before investing in solutions

#### ‚úÖ Positive Feedback
- Semantic search received positive reactions (demonstrated during demos)

---

## Feedback Template

### Date: [YYYY-MM-DD]

### Source: [Demo / User Interview / Usage Analytics / Support Request]

#### üéØ Key Feedback
- What did users say?
- What problems did they identify?
- What features did they request?

#### üí° Insights
- What patterns are emerging?
- What assumptions were validated/invalidated?
- What surprised you?

#### üìä Impact
- How critical is this feedback?
- How many users mentioned this?
- Does this align with other feedback?

#### ‚úÖ Positive Feedback
- What's working well?
- What delighted users?

---

## Feedback Patterns & Trends

*This section will be updated as patterns emerge across multiple feedback sessions*

### Emerging Themes
- [ ] TBD after collecting more feedback

### Feature Requests by Frequency
- [ ] TBD after collecting more feedback

### Validated Assumptions
- ‚úÖ Semantic search is a "wow factor" feature

### Invalidated Assumptions
- ‚ùå TBD

---

## Action Items

- [ ] Collect feedback from at least 5 more demos before prioritizing solutions
- [ ] Track which pain points are mentioned most frequently
- [ ] Identify whether friction or value-realization is the bigger blocker
- [ ] Validate passive detection concept with users (privacy/monitoring concerns)

---

## Notes

- Keep this document updated after each demo, user interview, or significant usage milestone
- Look for patterns across 5-10 feedback sessions before making major product decisions
- Prioritize solutions that address the most frequently mentioned problems
